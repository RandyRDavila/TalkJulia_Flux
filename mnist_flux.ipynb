{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming a Deep Neural Network with Flux\n",
    "\n",
    "## MNIST Data Set\n",
    "\n",
    "The MNIST data set consists of 70,000 images of hand written digits, 60,000 of which are typically used as labeled training examples and the other 10,000 used for testing your learning model on. The following picture represent a sample of some of the images.\n",
    "\n",
    "<img src=\"MnistExamples.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can load this dataset with the ```MLDatasets.jl``` package. Load this data by running the following code.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets, Flux\n",
    "using Plots, Images\n",
    "using Statistics\n",
    "\n",
    "\n",
    "# load full training set\n",
    "train_x, train_y = MNIST.traindata(Float32)\n",
    "\n",
    "# load full test set\n",
    "test_x,  test_y  = MNIST.testdata(Float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Each image is comprised of a $28\\times 28$ grey scaled grid of pixel values. These values are floating point numbers in the interval $(0,1)$, where darker pixels will have values closer to $1$ and lighter pixels will have values closer to $0$. The following image represents one such example. \n",
    "\n",
    "<img src=\"MNIST-Matrix.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "We can view the image of one of these matrices by running the following code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images.colorview\n",
    "colorview(Gray, train_x[:, :, 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Tensors are simply multi-dimensional matrices. The data structures ```train_x``` and ```test_x``` are stored as 3 dimensional tensors. \n",
    "\n",
    "<img src=\"order-3-tensor.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "This can be varified by viewing the size of these variables. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dimensions of the training data \n",
    "@show size(train_x)\n",
    "@show size(train_y)\n",
    "println()\n",
    "\n",
    "# Show the dimensions of the testing data\n",
    "@show size(test_x)\n",
    "@show size(test_y)\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Image Flattening\n",
    "\n",
    "Simple **dense neural networks** take as input feature vectors which are column vectors. In order to feed our images into such a network we must **flatten** the matrix into a column vector.\n",
    "\n",
    "<img src=\"flatten.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can do this for each image matrix we are considering by calling the ```Flux.flatten()```. Note, that $784 = 28 \\times 28$. By running the following code we reshape our images and store them in new variables. \n",
    "\n",
    "## One-Hot Encoding \n",
    "\n",
    "<img src=\"onehot.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "## Row Features and Column Instances \n",
    "Unlike most Python machine learning API's, Flux.jl and other Julia machine learning API's, we will store our training and testing data with feature measurements in the rows of our arrays and columns being instances. In the case of our 3-dimensional tensor, the 3rd dimension represents the number of instances of our data, i.e., the number of digits . \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Data in order to flatten each image into a linear array\n",
    "xtrain = Flux.flatten(train_x)\n",
    "xtest = Flux.flatten(test_x)\n",
    "\n",
    "# One-hot-encode the labels\n",
    "ytrain, ytest = Flux.onehotbatch(train_y, 0:9), Flux.onehotbatch(test_y, 0:9)\n",
    "\n",
    "# Print the dimensions of training feature matrices and training label matrices\n",
    "println(\"xtrain dimensions = $(size(xtrain))\")\n",
    "println(\"ytrain dimensions = $(size(ytrain))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of train_x\n",
    "(m, n, z) = size(train_x)\n",
    "\n",
    "# Chain together functions!\n",
    "model = Flux.Chain(\n",
    "                    Dense(m*n, 60, Flux.σ),\n",
    "                    Dense(60, 60, Flux.σ),\n",
    "                    Dense(60, 60, Flux.σ),\n",
    "                    Dense(60, 10, Flux.σ),\n",
    "                )\n",
    "\n",
    "# Define mean squared error loss function\n",
    "loss(x, y) = Flux.Losses.mse(model(x), y)\n",
    "\n",
    "# Define the accuracy \n",
    "accuracy(x, y) = Statistics.mean(Flux.onecold(model(x)) .== Flux.onecold(y))\n",
    "\n",
    "# ADAM would be the perferred optimizer for serious deep learning\n",
    "#opt = Flux.ADAM()\n",
    "\n",
    "# Define gradient descent optimizer\n",
    "# Flux.Descent\n",
    "opt = Descent(0.23)\n",
    "\n",
    "# Format your data\n",
    "data = [(xtrain, ytrain)]\n",
    "\n",
    "# Collect weights and bias for your model\n",
    "parameters = Flux.params(model)\n",
    "\n",
    "println(\"Old Loss = $(loss(xtrain, ytrain))\")\n",
    "println(\"Old Accuracy = $(accuracy(xtrain, ytrain)) \\n\")\n",
    "\n",
    "# Train the model over one epoch\n",
    "Flux.train!(loss, parameters, data, opt)\n",
    "\n",
    "\n",
    "println(\"New Loss = $(loss(xtrain, ytrain))\")\n",
    "println(\"New Accuracy = $(accuracy(xtrain, ytrain))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Flux: @epochs\n",
    "\n",
    "println(\"Old Loss = $(loss(xtrain, ytrain))\")\n",
    "println(\"Old Accuracy = $(accuracy(xtrain, ytrain)) \\n\")\n",
    "\n",
    "(m, n) = size(xtrain)\n",
    "\n",
    "# Train the model over 100_000 epochs\n",
    "for epoch in 1:100_000\n",
    "    # Randomly select a entry of training data \n",
    "    i = rand(1:n)\n",
    "    data = [(xtrain[:, i], ytrain[:, i])]\n",
    "\n",
    "    # Implement Stochastic Gradient Descent \n",
    "    Flux.train!(loss, parameters, data, opt)\n",
    "\n",
    "    # Print loss function values \n",
    "    if epoch % 10_000 == 0\n",
    "        println(\"Epoch: $(epoch)\")\n",
    "        @show loss(xtrain, ytrain)\n",
    "        @show accuracy(xtrain, ytrain)\n",
    "        println()\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = rand(1:1_000)\n",
    "\n",
    "predict(i) = argmax(model(xtest[:, i])) - 1\n",
    "\n",
    "digit = predict(i)\n",
    "println(\"Predict digit: $(digit)\")\n",
    "println(argmax(ytest[:, i]) - 1)\n",
    "\n",
    "colorview(Gray, test_x[:,:,i]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = rand(1:1_000)\n",
    "\n",
    "predict(i) = argmax(model(xtest[:, i])) - 1\n",
    "\n",
    "digit = predict(i)\n",
    "println(\"Predict digit: $(digit)\")\n",
    "println(argmax(ytest[:, i]) - 1)\n",
    "\n",
    "colorview(Gray, test_x[:,:,i]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TensorBoardLogger, Logging\n",
    "\n",
    "# Create tensorboard logger\n",
    "# TensorBoardLogger.TBLogger\n",
    "logger = TBLogger(\"content/log\", tb_overwrite)\n",
    "\n",
    "\n",
    "# Log some images as samples (not needed)\n",
    "with_logger(logger) do\n",
    "    images = TBImage(train_x[:, :, 1:10], WHN)\n",
    "    @info \"mnist/samples\" pics = images log_step_increment=0\n",
    "end\n",
    "\n",
    "# Function to get dictionary of model parameters\n",
    "function fill_param_dict!(dict, m, prefix)\n",
    "    if m isa Chain\n",
    "        for (i, layer) in enumerate(m.layers)\n",
    "            fill_param_dict!(dict, layer, prefix*\"layer_\"*string(i)*\"/\"*string(layer)*\"/\")\n",
    "        end\n",
    "    else\n",
    "        for fieldname in fieldnames(typeof(m))\n",
    "            val = getfield(m, fieldname)\n",
    "            if val isa AbstractArray\n",
    "                val = vec(val)\n",
    "            end\n",
    "            dict[prefix*string(fieldname)] = val\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# Callback to log information after every epoch\n",
    "function TBCallback()\n",
    "    param_dict = Dict{String, Any}()\n",
    "    fill_param_dict!(param_dict, model, \"\")\n",
    "    with_logger(logger) do\n",
    "      @info \"model\" params=param_dict log_step_increment=0\n",
    "      @info \"train\" loss=loss(xtrain, ytrain) acc=accuracy(xtrain, ytrain) log_step_increment=0\n",
    "      @info \"test\" loss=loss(xtest, ytest) acc=accuracy(xtest, ytest)\n",
    "    end\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of train_x\n",
    "(m, n, z) = size(train_x)\n",
    "\n",
    "# Chain together functions!\n",
    "model = Flux.Chain(\n",
    "                    Dense(m*n, 60, Flux.σ),\n",
    "                    Dense(60, 60, Flux.σ),\n",
    "                    Dense(60, 60, Flux.σ),\n",
    "                    Dense(60, 10, Flux.σ),\n",
    "                )\n",
    "\n",
    "# Define mean squared error loss function\n",
    "loss(x, y) = Flux.Losses.mse(model(x), y)\n",
    "\n",
    "# Define the accuracy \n",
    "accuracy(x, y) = Statistics.mean(Flux.onecold(model(x) |> cpu) .== Flux.onecold(y |> cpu))\n",
    "\n",
    "# ADAM would be the perferred optimizer for serious deep learning\n",
    "opt = Flux.ADAM()\n",
    "\n",
    "\n",
    "# Format your data\n",
    "data = [(xtrain, ytrain)]\n",
    "\n",
    "# Collect weights and bias for your model\n",
    "parameters = Flux.params(model)\n",
    "\n",
    "println(\"Old Loss = $(loss(xtrain, ytrain))\")\n",
    "\n",
    "# Train the model over 100 epochs\n",
    "for epoch in 1:100\n",
    "    Flux.train!(loss, parameters, data, opt, cb = Flux.throttle(TBCallback, 5))\n",
    "end\n",
    "\n",
    "\n",
    "println(\"New Loss = $(loss(xtrain, ytrain))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
